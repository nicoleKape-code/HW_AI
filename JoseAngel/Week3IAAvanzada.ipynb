{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IdG9MML3h7M"
      },
      "source": [
        "# Dense Layers\n",
        "\n",
        "La capa densa, también llamada capa completamente conectada,y se utiliza para representaciones abstractas de los datos de entrada. En esta capa, las neuronas se conectan con todas las neuronas de la capa anterior. En las redes perceptrónicas multicapa, estas capas se apilan.\n",
        "\n",
        "## Problemas Comunes de las Dense Layers.\n",
        "\n",
        "## 1. Sobreajuste (Overfitting)\n",
        "El sobreajuste ocurre cuando el modelo se ajusta demasiado bien a los datos de entrenamiento, capturando el ruido y los detalles específicos de esos datos. Esto lleva a un mal rendimiento en nuevos datos no vistos. Las dense layers, debido a su gran capacidad de representación, son particularmente susceptibles al sobreajuste.\n",
        "\n",
        "## 2. Sobrecarga Computacional\n",
        "Las dense layers requieren una gran cantidad de parámetros, especialmente cuando se utilizan muchas neuronas. Esto puede resultar en un alto costo computacional tanto en términos de tiempo como de memoria durante el entrenamiento y la inferencia.\n",
        "\n",
        "## 3. Vanishing Gradients\n",
        "En redes profundas, el problema del desvanecimiento del gradiente puede ser severo. Esto pasa cuando los gradientes se vuelven extremadamente pequeños a medida que se propagan hacia atrás a través de las capas, ralentizando o incluso deteniendo el entrenamiento. Esto es especialmente problemático para las dense layers en redes muy profundas.\n",
        "\n",
        "## 4. Exploding Gradients\n",
        "Lo contrario del desvanecimiento del gradiente. Aquí, los gradientes se vuelven muy grandes y causan que los pesos se actualicen de forma abrupta, lo que puede hacer que el modelo se vuelva inestable.\n",
        "\n",
        "## 5. Ineficiencia en la Representación de Datos Espaciales\n",
        "Las dense layers no están diseñadas para manejar datos espaciales de manera eficiente. En tareas como el procesamiento de imágenes, donde las convoluciones son más efectivas, las dense layers tienden a no capturar bien las estructuras espaciales, resultando en un desempeño subóptimo.\n",
        "\n",
        "## 6. Necesidad de Regularización\n",
        "Debido a su tendencia al sobreajuste, las dense layers a menudo requieren métodos de regularización como Dropout, L2 regularization, entre otros, para mejorar su rendimiento en datos no vistos.\n",
        "\n",
        "## 7. Dificultades en la Interpretación\n",
        "Las dense layers, y en general las redes profundas, son difíciles de interpretar. No es sencillo entender qué características están siendo capturadas o cómo las decisiones están siendo tomadas, lo cual puede ser un problema en aplicaciones que requieren transparencia y explicabilidad.\n",
        "\n",
        "## 8. Sensibilidad a la Inicialización de Pesos\n",
        "La performance de las dense layers puede variar significativamente dependiendo de cómo se inicialicen sus pesos. Una mala inicialización puede llevar a un entrenamiento lento o a la convergencia en un mínimo local de baja calidad.\n",
        "\n",
        "## 9. Dependencia del Tamaño del Lote (Batch Size)\n",
        "El tamaño del lote utilizado durante el entrenamiento puede afectar fuertemente el rendimiento de las dense layers. Un tamaño de lote muy pequeño puede resultar en un entrenamiento ruidoso y una convergencia lenta, mientras que un tamaño de lote muy grande puede sobrecargar la memoria y también llevar al modelo a mínimos locales pobres.\n",
        "\n",
        "# CNN\n",
        "\n",
        "Una Convolutional Neural Network (CNN) es un tipo de red neuronal artificial que está diseñada para procesar datos con una topología de grilla (como una imagen) mediante el uso de operaciones de convolución. Las CNNs son particularmente efectivas para tareas de percepción visual, como clasificación de imágenes, detección de objetos y segmentación de imágenes.\n",
        "\n",
        "## Arquitectura de una CNN\n",
        "\n",
        "La arquitectura de una CNN está compuesta por varias capas, las más comunes son:\n",
        "\n",
        "### 1. Capas Convolucionales (Convolutional Layers)\n",
        "- **Descripción**: Estas capas aplican un conjunto de filtros (kernels) a las entradas para extraer características locales. Los filtros se deslizan sobre la entrada y realizan operaciones de convolución, generando mapas de características.\n",
        "- **Función**: Capturan características locales como bordes, texturas y patrones pequeños al principio, y estructuras más complejas en capas más profundas.\n",
        "\n",
        "### 2. Capas de Pooling (Pooling Layers)\n",
        "- **Descripción**: Reducen la dimensionalidad de los mapas de características mediante operaciones como max pooling o average pooling.\n",
        "- **Función**: Resumen las características fundamentales, reducen la cantidad de parámetros y controlan el sobreajuste.\n",
        "\n",
        "### 3. Capas de Normalización (Normalization Layers)\n",
        "- **Descripción**: Aparecen a veces después de las capas convolucionales para normalizar la activación. Un ejemplo es la Batch Normalization.\n",
        "- **Función**: Facilitan el entrenamiento al acelerar la convergencia y estabilizar el proceso de aprendizaje.\n",
        "\n",
        "### 4. Capas Densas (Fully Connected Layers)\n",
        "- **Descripción**: Al final de la red, estas capas conectan todas las neuronas de la capa anterior con cada neurona de la capa siguiente.\n",
        "- **Función**: Realizan la combinación final de las características extraídas para generar la salida del modelo, como la clasificación final de una imagen.\n",
        "\n",
        "\n",
        "## Características Clave de las CNNs\n",
        "\n",
        "### Receptive Field (Campo Receptivo)\n",
        "Cada neurona en una capa convolucional está conectada solo a una región pequeña de la capa anterior, llamada campo receptivo, lo que permite capturar características locales.\n",
        "\n",
        "### Compartición de Pesos (Weight Sharing)\n",
        "Los mismos filtros (pesos) se aplican en diferentes partes de la entrada, lo que reduce la cantidad de parámetros y mejora la eficiencia y generalización.\n",
        "\n",
        "### Hierarquía de Características\n",
        "Las CNNs construyen una jerarquía de características desde las más simples hasta las más complejas, lo que es útil para tareas de reconocimiento.\n",
        "\n",
        "## Ventajas de las CNNs\n",
        "\n",
        "- **Eficiencia en el Procesamiento de Imágenes**: Al explotar la estructura espacial de las imágenes, las CNNs pueden aprender patrones complejos con menos parámetros.\n",
        "- **Reducción de la Dimensionalidad**: Mediante técnicas como el pooling, las CNNs reducen la dimensionalidad de las características, disminuyendo la carga computacional.\n",
        "- **Generalización**: La compartición de pesos y la capacidad de los filtros para detectar características en cualquier parte de la imagen mejoran la capacidad de generalización del modelo.\n",
        "\n",
        "## Aplicaciones de las CNNs\n",
        "\n",
        "- **Clasificación de Imágenes**: Identificar y categorizar objetos dentro de una imagen.\n",
        "- **Detección de Objetos**: Localizar y clasificar múltiples objetos dentro de una imagen.\n",
        "- **Segmentación de Imágenes**: Asignar una etiqueta a cada píxel de la imagen, diferenciando las diversas regiones o objetos.\n",
        "- **Reconocimiento Facial**: Identificar y verificar rostros humanos en imágenes.\n",
        "- **Visión en Vehículos Autónomos**: Interpretar el entorno a través de cámaras para la navegación segura.\n",
        "\n",
        "\n",
        "## Arquitecturas Más Comunes de CNN\n",
        "\n",
        "- **LeNet (LeNet-5)**\n",
        "  - **Descripción**: Propuesta por Yann LeCun en los años 90, LeNet-5 es una de las primeras CNNs exitosas y se utilizó principalmente para el reconocimiento de caracteres escritos a mano, como los dígitos del MNIST.\n",
        "  - **Arquitectura**:\n",
        "    - Dos capas convolucionales seguidas por capas de pooling.\n",
        "    - Capas densas totalmente conectadas.\n",
        "    - Función de activación sigmoide o tanh.\n",
        "\n",
        "- **AlexNet**\n",
        "  - **Descripción**: Propuesta por Alex Krizhevsky et al. en 2012, ganó el ImageNet Large Scale Visual Recognition Challenge (ILSVRC) ese año. Fue una de las primeras redes en aprovechar GPU para el entrenamiento a gran escala.\n",
        "  - **Arquitectura**:\n",
        "    - Cinco capas convolucionales seguidas por capas de pooling.\n",
        "    - Capas totalmente conectadas con ReLU como función de activación.\n",
        "    - Uso de Dropout para regularización.\n",
        "\n",
        "- **VGGNet (VGG16/VGG19)**\n",
        "  - **Descripción**: Propuesta por el grupo de Visual Geometry Group (VGG) de la Universidad de Oxford en 2014. Conocida por su simplicidad y su profundidad, utilizó solo convoluciones de 3x3 píxeles.\n",
        "  - **Arquitectura**:\n",
        "    - Series de capas convolucionales de 3x3 seguidas por capas de pooling.\n",
        "    - Capas totalmente conectadas al final.\n",
        "    - VGG16 tiene 16 capas de peso y VGG19 tiene 19.\n",
        "\n",
        "- **GoogLeNet (Inception)**\n",
        "  - **Descripción**: Propuesta por Christian Szegedy et al. de Google en 2014 y ganador del ILSVRC de ese año. Introduce el bloque Inception que permite varias convoluciones en paralelo.\n",
        "  - **Arquitectura**:\n",
        "    - Uso de módulos Inception que aplican convoluciones de diferentes tamaños (1x1, 3x3, 5x5) y pooling en paralelo.\n",
        "    - Red más profunda y ancha con menos parámetros comparado con AlexNet y VGG.\n",
        "\n",
        "- **ResNet (Residual Networks)**\n",
        "  - **Descripción**: Propuesta por Kaiming He et al. en 2015, ganó el ILSVRC. Introduce el concepto de residual learning para permitir la construcción de redes extremadamente profundas.\n",
        "  - **Arquitectura**:\n",
        "    - Uso de bloques residuales que permiten shortcuts o saltos de conexión.\n",
        "    - Red muy profunda con versiones como ResNet-50, ResNet-101 y ResNet-152.\n",
        "\n",
        "- **DenseNet (Densely Connected Networks)**\n",
        "  - **Descripción**: Propuesta por Gao Huang et al. en 2017. Las densas conexiones entre capas mejoran el flujo de gradientes y reutilizan características.\n",
        "  - **Arquitectura**:\n",
        "    - Cada capa recibe entradas de todas las capas anteriores y pasa su salida a todas las capas siguientes.\n",
        "    - Fortalecen el flujo de información y gradiente.\n",
        "\n",
        "- **MobileNet**\n",
        "  - **Descripción**: Propuesta por Google, diseñada para dispositivos móviles y sistemas embebidos. Usa convoluciones separables en profundidad para reducir el costo computacional.\n",
        "  - **Arquitectura**:\n",
        "    - Uso de convoluciones separables en profundidad (depthwise separable convolutions).\n",
        "    - Variante ligera y eficiente en términos de parámetros y operaciones.\n",
        "\n",
        "- **EfficientNet**\n",
        "  - **Descripción**: Propuesta por Mingxing Tan y Quoc V. Le en 2019. Introduce un método sistemático para escalar las arquitecturas de las redes.\n",
        "  - **Arquitectura**:\n",
        "    - Modelos escalados de manera eficiente en términos de profundidad, anchura y resolución.\n",
        "    - Mejor relación precisión-eficiencia.\n",
        "\n",
        "\n",
        "##Ejemplo de CNN con KERAS\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9ebOMMyj934Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu6dt79f-JPV",
        "outputId": "c918147b-c444-4dfb-9511-17d2c3883c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test)= keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "3WfL3Oa9-Q9X",
        "outputId": "1778a873-f2d7-438e-fb39-cb0a6bc27280"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m16,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,826\u001b[0m (136.04 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,826\u001b[0m (136.04 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmIT-iVN-YDF",
        "outputId": "c943f638-0054-45da-8c18-f27678a21a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.7539 - loss: 0.6899 - val_accuracy: 0.8505 - val_loss: 0.4350\n",
            "Epoch 2/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8400 - loss: 0.4442 - val_accuracy: 0.8648 - val_loss: 0.3793\n",
            "Epoch 3/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8591 - loss: 0.3958 - val_accuracy: 0.8682 - val_loss: 0.3565\n",
            "Epoch 4/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8684 - loss: 0.3680 - val_accuracy: 0.8822 - val_loss: 0.3236\n",
            "Epoch 5/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8752 - loss: 0.3498 - val_accuracy: 0.8838 - val_loss: 0.3116\n",
            "Epoch 6/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8796 - loss: 0.3317 - val_accuracy: 0.8900 - val_loss: 0.2995\n",
            "Epoch 7/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8839 - loss: 0.3206 - val_accuracy: 0.8913 - val_loss: 0.2888\n",
            "Epoch 8/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8872 - loss: 0.3093 - val_accuracy: 0.8888 - val_loss: 0.2943\n",
            "Epoch 9/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8914 - loss: 0.3017 - val_accuracy: 0.8893 - val_loss: 0.2940\n",
            "Epoch 10/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8955 - loss: 0.2922 - val_accuracy: 0.8948 - val_loss: 0.2792\n",
            "Epoch 11/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8953 - loss: 0.2872 - val_accuracy: 0.8875 - val_loss: 0.2929\n",
            "Epoch 12/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8977 - loss: 0.2819 - val_accuracy: 0.9038 - val_loss: 0.2614\n",
            "Epoch 13/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9003 - loss: 0.2761 - val_accuracy: 0.9035 - val_loss: 0.2576\n",
            "Epoch 14/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9002 - loss: 0.2733 - val_accuracy: 0.9072 - val_loss: 0.2539\n",
            "Epoch 15/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9025 - loss: 0.2683 - val_accuracy: 0.9077 - val_loss: 0.2533\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x317641790>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 128\n",
        "epochs = 15\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCqH4kQy-aUF",
        "outputId": "69cb43f4-7874-4a98-db3f-d2d33d2c3c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.2714228630065918\n",
            "Test accuracy: 0.8992000222206116\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xfA6bsA-bFc"
      },
      "source": [
        "## Ejercicio de clase\n",
        "\n",
        "En equipos, haga una CNN que pueda clasificar el set de [Fashion-MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist) utilizando Keras.\n",
        "\n",
        "\n",
        "# Data Augmentation\n",
        "\n",
        "## ¿Qué es Data Augmentation?\n",
        "\n",
        "El **Data Augmentation**, o aumento de datos, es una técnica utilizada en el campo del aprendizaje automático para aumentar la cantidad de datos de entrenamiento disponibles mediante la creación de nuevas muestras a partir de los datos existentes. Esto se logra aplicando diversas transformaciones a los datos originales, creando así una versión ampliada y diversa del conjunto de datos.\n",
        "\n",
        "## ¿Por qué es importante?\n",
        "\n",
        "1. **Mejora del Rendimiento del Modelo**: Al tener más datos disponibles, los modelos pueden aprender patrones más robustos y mejorar su precisión y generalización.\n",
        "2. **Reducción del Overfitting**: Aumentar el tamaño del conjunto de datos ayuda a reducir el riesgo de sobreajuste (overfitting), donde un modelo aprende demasiado sobre los datos de entrenamiento y falla al generalizar a datos nuevos.\n",
        "3. **Mayor diversidad**: Las transformaciones pueden introducir variaciones que los datos originales no tienen, ayudando así al modelo a ser más robusto frente a diversas situaciones de datos en el mundo real.\n",
        "\n",
        "## Métodos Comunes de Data Augmentation\n",
        "\n",
        "### Imágenes\n",
        "\n",
        "1. **Rotación**: Girar la imagen en varios ángulos.\n",
        "2. **Escalado**: Aumentar o disminuir el tamaño de la imagen.\n",
        "3. **Traslación**: Desplazar la imagen en el eje X o Y.\n",
        "4. **Volteo**: Voltear la imagen horizontal o verticalmente.\n",
        "5. **Ajuste de Brillo y Contraste**: Modificar la iluminación y el contraste.\n",
        "6. **Ruido Aleatorio**: Añadir ruido a la imagen para hacerla más robusta a imperfecciones.\n",
        "\n",
        "### Audio\n",
        "\n",
        "1. **Cambio de Tonalidad**: Modificar la frecuencia de la señal de audio.\n",
        "2. **Agregación de Ruido de Fondo**: Añadir ruido de fondo para hacer la señal más realista.\n",
        "3. **Estiramiento Temporal**: Alterar la velocidad de reproducción sin cambiar la tonalidad.\n",
        "4. **Transformaciones de Fourier**: Aplicar modificaciones en el dominio de la frecuencia.\n",
        "\n",
        "### Texto\n",
        "\n",
        "1. **Sinónimos**: Reemplazar palabras con sus sinónimos.\n",
        "2. **Insertar Palabras Aleatorias**: Añadir palabras al azar dentro del texto.\n",
        "3. **Eliminar Palabras Aleatorias**: Quitar algunas palabras del texto.\n",
        "4. **Parafraseo**: Reformular oraciones manteniendo el mismo significado.\n",
        "\n",
        "## Herramientas y Bibliotecas\n",
        "\n",
        "- **TensorFlow**: Ofrece funciones de data augmentation como parte de su API de transformación de datos.\n",
        "- **Keras**: Incluye el `ImageDataGenerator` que proporciona muchas transformaciones para imágenes.\n",
        "- **Albumentations**: Popular biblioteca para aumentación de imágenes en proyectos de visión por computadora.\n",
        "- **nltk** y **Spacy**: Para tareas de NLP, estas bibliotecas pueden ser útiles para data augmentation textual.\n",
        "- **Audiomentations**: Biblioteca específica para la aumentación de datos de audio.\n",
        "\n",
        "## Ejemplo en Keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "GlUa1E6j-dkQ",
        "outputId": "29aeae57-bce2-4386-b9f1-3f7f8fd6c3ef"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# suponiendo que `train_images` son tus imágenes de entrenamiento\n",
        "datagen.fit(train_images)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
